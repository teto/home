version = 1

; Preset for cached HuggingFace model
; [ggml-org/gemma-3-27b-it-GGUF:Q6_K]
; chat-template = chatml
; ngl = 123
; jinja = on
; ctx-size = 8192


[mistral-7b]
m = mistral-7b-v0.1.Q8_0.gguf

; Custom local model with absolute path
[ministral3-14b]
; m = Ministral-3-14B-Base-2512.Q6_K.gguf
m = Ministral-3-14B-Base-2512.Q6_K-weird-prompt-doesntwork.gguf
; mmproj = Ministral-3-14B-Base-2512.Q6_K.gguf
ctx-size = 8192
; temp = 0.7
; top-p = 0.8

; doesnt work well
; [ministral3-3b-q4]
; m = mistralai_Ministral-3-3B-Instruct-2512-GGUF_Ministral-3-3B-Instruct-2512-Q4_K_M.gguf

[devstral2-24b-iq2]
m = mistralai_Devstral-Small-2-24B-Instruct-2512-IQ2_M.gguf
ctx-size = 8192

[qwen2.5-3b-coder]
m = ggml-org_Qwen2.5-Coder-3B-Q8_0-GGUF_qwen2.5-coder-3b-q8_0.gguf 

; MoE model with specific settings
; [MoE-Qwen3-30B-A3B-Thinking]
; m = ./
; n-cpu-moe = 30
; temp = 0.6
; top-p = 0.95
; ctx-size = 8192
