; one could eventually use https://github.com/ggml-org/llama.cpp/blob/master/docs/preset.md#named-presets
version = 1

; Preset for cached HuggingFace model
; [ggml-org/gemma-3-27b-it-GGUF:Q6_K]
; chat-template = chatml
; ngl = 123
; jinja = on
; ctx-size = 8192

[*]
; useless
models-dir = /home/teto/llama-models


[mistral-7b]
; we need v 3
m = /home/teto/llama-models/mistral-7b-v0.1.Q8_0.gguf
; models-dir = 

; Custom local model with absolute path
[ministral3-14b]
; m = Ministral-3-14B-Base-2512.Q6_K.gguf
m = /home/teto/llama-models/Ministral-3-14B-Base-2512.Q6_K-weird-prompt-doesntwork.gguf
; mmproj = Ministral-3-14B-Base-2512.Q6_K.gguf
ctx-size = 8192
; temp = 0.7
; top-p = 0.8
chat-template-kwargs = {"reasoning_effort": "high"}

[ministral3-3b]
m = /home/teto/llama-models/mistralai_Ministral-3-3B-Instruct-2512-GGUF_Ministral-3-3B-Instruct-2512-Q4_K_M.gguf
ctx-size = 8192

[ministral3-8b]
m = /home/teto/llama-models/Ministral-3-8B-Instruct-2512-UD-Q6_K_XL.gguf 
ctx-size = 8192

; doesnt work well
; [ministral3-3b-q4]
; m = mistralai_Ministral-3-3B-Instruct-2512-GGUF_Ministral-3-3B-Instruct-2512-Q4_K_M.gguf

[devstral2-24b-iq2]
m = /home/teto/llama-models/mistralai_Devstral-Small-2-24B-Instruct-2512-IQ2_M.gguf
ctx-size = 8192

[qwen2.5-3b-coder]
m = /home/teto/llama-models/ggml-org_Qwen2.5-Coder-3B-Q8_0-GGUF_qwen2.5-coder-3b-q8_0.gguf 

[qwen2.5-7b-coder]
m = /home/teto/llama-models/qwen2.5-coder-7b-instruct-q8_0.gguf

; MoE model with specific settings
; [MoE-Qwen3-30B-A3B-Thinking]
; m = ./
; n-cpu-moe = 30
; temp = 0.6
; top-p = 0.95
; ctx-size = 8192
