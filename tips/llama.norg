You need a GGUF model file in new versions.

Cache at /home/teto/.cache/llama.cpp

with Llama.cpp you can still load SOME experts on GPU by reducing --n-cpu-moe
